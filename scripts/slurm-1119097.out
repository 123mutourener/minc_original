Running SLURM prolog script on alpha51.cluster.local
===============================================================================
Job started on Sat Dec 19 17:50:01 GMT 2020
Job ID          : 1119097
Job name        : pl_alpha_patch.sh
WorkDir         : /mainfs/home/yh1n19/minc_original/scripts
Command         : /mainfs/home/yh1n19/minc_original/scripts/pl_alpha_patch.sh
Partition       : ecsstaff
Num hosts       : 1
Num cores       : 4
Num of tasks    : 4
Hosts allocated : alpha51
Job Output Follows ...
===============================================================================
no change     /local/software/conda/miniconda-py3-new/condabin/conda
no change     /local/software/conda/miniconda-py3-new/bin/conda
no change     /local/software/conda/miniconda-py3-new/bin/conda-env
no change     /local/software/conda/miniconda-py3-new/bin/activate
no change     /local/software/conda/miniconda-py3-new/bin/deactivate
no change     /local/software/conda/miniconda-py3-new/etc/profile.d/conda.sh
no change     /local/software/conda/miniconda-py3-new/etc/fish/conf.d/conda.fish
no change     /local/software/conda/miniconda-py3-new/shell/condabin/Conda.psm1
no change     /local/software/conda/miniconda-py3-new/shell/condabin/conda-hook.ps1
no change     /local/software/conda/miniconda-py3-new/lib/python3.7/site-packages/xontrib/conda.xsh
no change     /local/software/conda/miniconda-py3-new/etc/profile.d/conda.csh
no change     /home/yh1n19/.bashrc
No action taken.
GPU available: True, used: True
TPU available: None, using: 0 TPU cores
Multi-processing is handled by Slurm.
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/4
GPU available: True, used: True
TPU available: None, using: 0 TPU cores
Multi-processing is handled by Slurm.
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/4
GPU available: True, used: True
TPU available: None, using: 0 TPU cores
GPU available: True, used: True
Multi-processing is handled by Slurm.
TPU available: None, using: 0 TPU cores
Multi-processing is handled by Slurm.
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/4
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/4
alpha51:112136:112136 [0] NCCL INFO Bootstrap : Using [0]eno1:10.13.37.51<0>
alpha51:112136:112136 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).

alpha51:112136:112136 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
alpha51:112136:112136 [0] NCCL INFO NET/Socket : Using [0]eno1:10.13.37.51<0>
NCCL version 2.4.8+cuda10.2
alpha51:112137:112137 [0] NCCL INFO Bootstrap : Using [0]eno1:10.13.37.51<0>
alpha51:112138:112138 [0] NCCL INFO Bootstrap : Using [0]eno1:10.13.37.51<0>
alpha51:112139:112139 [0] NCCL INFO Bootstrap : Using [0]eno1:10.13.37.51<0>
alpha51:112138:112138 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
alpha51:112137:112137 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
alpha51:112139:112139 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).

alpha51:112138:112138 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

alpha51:112137:112137 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

alpha51:112139:112139 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
alpha51:112137:112137 [0] NCCL INFO NET/Socket : Using [0]eno1:10.13.37.51<0>
alpha51:112138:112138 [0] NCCL INFO NET/Socket : Using [0]eno1:10.13.37.51<0>
alpha51:112139:112139 [0] NCCL INFO NET/Socket : Using [0]eno1:10.13.37.51<0>
alpha51:112136:112193 [0] NCCL INFO Setting affinity for GPU 0 to 01
alpha51:112137:112196 [0] NCCL INFO Setting affinity for GPU 0 to 01,00000000
alpha51:112138:112194 [0] NCCL INFO Setting affinity for GPU 0 to 10
alpha51:112139:112195 [0] NCCL INFO Setting affinity for GPU 0 to 10,00000000
alpha51:112136:112193 [0] NCCL INFO Channel 00 :    0   1   2   3
alpha51:112137:112196 [0] NCCL INFO Ring 00 : 1[0] -> 2[0] via P2P/IPC
alpha51:112138:112194 [0] NCCL INFO Ring 00 : 2[0] -> 3[0] via P2P/IPC
alpha51:112139:112195 [0] NCCL INFO Ring 00 : 3[0] -> 0[0] via P2P/IPC
alpha51:112136:112193 [0] NCCL INFO Ring 00 : 0[0] -> 1[0] via P2P/IPC
alpha51:112136:112193 [0] NCCL INFO Using 256 threads, Min Comp Cap 7, Trees disabled
alpha51:112138:112194 [0] NCCL INFO comm 0x2b0738001d60 rank 2 nranks 4 cudaDev 0 nvmlDev 0 - Init COMPLETE
alpha51:112139:112195 [0] NCCL INFO comm 0x2ac59c001d60 rank 3 nranks 4 cudaDev 0 nvmlDev 0 - Init COMPLETE
alpha51:112136:112193 [0] NCCL INFO comm 0x2b8bd8001d60 rank 0 nranks 4 cudaDev 0 nvmlDev 0 - Init COMPLETE
alpha51:112137:112196 [0] NCCL INFO comm 0x2acdcc001d60 rank 1 nranks 4 cudaDev 0 nvmlDev 0 - Init COMPLETE
alpha51:112136:112136 [0] NCCL INFO Launch mode Parallel
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.

  | Name       | Type         | Params
--------------------------------------------
0 | net        | DataParallel | 7.0 M 
1 | train_acc  | Accuracy     | 0     
2 | valid_acc  | Accuracy     | 0     
3 | test_acc   | Accuracy     | 0     
4 | train_loss | Loss         | 0     
5 | valid_loss | Loss         | 0     
6 | test_loss  | Loss         | 0     
--------------------------------------------
7.0 M     Trainable params
0         Non-trainable params
7.0 M     Total params
Start the patch experiment
Network densenet121 loaded successfully
GPU mode enabled with 4 chips
Validation set loaded, with 179795 samples
NCCL version 2.4.8+cuda10.2
alpha51:112137:112137 [0] NCCL INFO nranks 4
alpha51:112137:112137 [0] NCCL INFO Setting affinity for GPU 0 to 01,00000000
alpha51:112137:112137 [1] NCCL INFO Setting affinity for GPU 1 to 01,00000000
alpha51:112137:112137 [3] NCCL INFO Using 256 threads, Min Comp Cap 7, Trees disabled
alpha51:112137:112137 [3] NCCL INFO Channel 00 :    0   1   2   3
alpha51:112137:112137 [0] NCCL INFO Ring 00 : 0[0] -> 1[1] via P2P/direct pointer
alpha51:112137:112137 [1] NCCL INFO Ring 00 : 1[1] -> 2[2] via direct shared memory
alpha51:112137:112137 [2] NCCL INFO Ring 00 : 2[2] -> 3[3] via P2P/direct pointer
alpha51:112137:112137 [3] NCCL INFO Ring 00 : 3[3] -> 0[0] via direct shared memory
alpha51:112137:112137 [0] NCCL INFO Launch mode Group/CGMD
NCCL version 2.4.8+cuda10.2
alpha51:112138:112138 [0] NCCL INFO nranks 4
alpha51:112138:112138 [0] NCCL INFO Setting affinity for GPU 0 to 10
alpha51:112138:112138 [1] NCCL INFO Setting affinity for GPU 1 to 10
alpha51:112138:112138 [3] NCCL INFO Using 256 threads, Min Comp Cap 7, Trees disabled
alpha51:112138:112138 [3] NCCL INFO Channel 00 :    0   1   2   3
alpha51:112138:112138 [0] NCCL INFO Ring 00 : 0[0] -> 1[1] via P2P/direct pointer
alpha51:112138:112138 [1] NCCL INFO Ring 00 : 1[1] -> 2[2] via direct shared memory
alpha51:112138:112138 [2] NCCL INFO Ring 00 : 2[2] -> 3[3] via P2P/direct pointer
alpha51:112138:112138 [3] NCCL INFO Ring 00 : 3[3] -> 0[0] via direct shared memory
alpha51:112138:112138 [0] NCCL INFO Launch mode Group/CGMD
Validation sanity check: 0it [00:00, ?it/s]alpha51:112136:112136 [0] NCCL INFO nranks 4
alpha51:112136:112136 [0] NCCL INFO Setting affinity for GPU 0 to 01
alpha51:112136:112136 [1] NCCL INFO Setting affinity for GPU 1 to 01
alpha51:112136:112136 [3] NCCL INFO Using 256 threads, Min Comp Cap 7, Trees disabled
alpha51:112136:112136 [3] NCCL INFO Channel 00 :    0   1   2   3
alpha51:112136:112136 [0] NCCL INFO Ring 00 : 0[0] -> 1[1] via P2P/direct pointer
alpha51:112136:112136 [1] NCCL INFO Ring 00 : 1[1] -> 2[2] via direct shared memory
alpha51:112136:112136 [2] NCCL INFO Ring 00 : 2[2] -> 3[3] via P2P/direct pointer
alpha51:112136:112136 [3] NCCL INFO Ring 00 : 3[3] -> 0[0] via direct shared memory
alpha51:112136:112136 [0] NCCL INFO Launch mode Group/CGMD
NCCL version 2.4.8+cuda10.2
alpha51:112139:112139 [0] NCCL INFO nranks 4
alpha51:112139:112139 [0] NCCL INFO Setting affinity for GPU 0 to 10,00000000
alpha51:112139:112139 [1] NCCL INFO Setting affinity for GPU 1 to 10,00000000
alpha51:112139:112139 [3] NCCL INFO Using 256 threads, Min Comp Cap 7, Trees disabled
alpha51:112139:112139 [3] NCCL INFO Channel 00 :    0   1   2   3
alpha51:112139:112139 [0] NCCL INFO Ring 00 : 0[0] -> 1[1] via P2P/direct pointer
alpha51:112139:112139 [1] NCCL INFO Ring 00 : 1[1] -> 2[2] via direct shared memory
alpha51:112139:112139 [2] NCCL INFO Ring 00 : 2[2] -> 3[3] via P2P/direct pointer
alpha51:112139:112139 [3] NCCL INFO Ring 00 : 3[3] -> 0[0] via direct shared memory
alpha51:112139:112139 [0] NCCL INFO Launch mode Group/CGMD
                                           Training set loaded, with 2517205 samples
srun: Job step aborted: Waiting up to 602 seconds for job step to finish.
slurmstepd: error: *** STEP 1119097.0 ON alpha51 CANCELLED AT 2020-12-19T18:04:45 ***
bypassing sigterm
slurmstepd: error: *** JOB 1119097 ON alpha51 CANCELLED AT 2020-12-19T18:04:45 ***
bypassing sigterm
/home/yh1n19/.conda/envs/cv-pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  warnings.warn(*args, **kwargs)
/home/yh1n19/.conda/envs/cv-pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  warnings.warn(*args, **kwargs)
bypassing sigterm
bypassing sigterm
==============================================================================
Running epilogue script on alpha51.

Submit time  : 2020-12-19T17:50:00
Start time   : 2020-12-19T17:50:00
End time     : 2020-12-19T18:04:45
Elapsed time : 00:14:45 (Timelimit=05:00:00)

Job ID: 1119097
Cluster: i5
User/Group: yh1n19/fp
State: CANCELLED (exit code 0)
Nodes: 1
Cores per node: 4
CPU Utilized: 01:15:45
CPU Efficiency: 128.39% of 00:59:00 core-walltime
Job Wall-clock time: 00:14:45
Memory Utilized: 22.09 GB (estimated maximum)
Memory Efficiency: 104.74% of 21.09 GB (5.27 GB/core)

